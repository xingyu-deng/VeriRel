{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c887387-babd-4b74-bb2a-80941402a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import copy\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8258062d-c719-4c85-a9eb-8c33cb618f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "def prob_processed_data(prob_result):\n",
    "    for item in prob_result:\n",
    "        evidence_data = []\n",
    "        for key, value in item['evidence'].items():\n",
    "            if 'probability' in value:\n",
    "                prob = value['probability']\n",
    "                prob = softmax(prob)\n",
    "                max_prob = 1 - prob[1]\n",
    "                evidence_data.append((key, max_prob))\n",
    "        \n",
    "        evidence_data_sorted = sorted(evidence_data, key=lambda x: x[1], reverse=True)\n",
    "        sorted_evidence = {key: item['evidence'][key] for key, _ in evidence_data_sorted}\n",
    "        item['evidence'] = sorted_evidence\n",
    "    \n",
    "    result_data_list = [{'id': item['id'], 'doc_ids': item['evidence']} for item in prob_result]\n",
    "    for item in result_data_list:\n",
    "        item['doc_ids'] = [int(key) for key in item['doc_ids'].keys()]\n",
    "    return result_data_list\n",
    "\n",
    "\n",
    "def mix_rerank_processed_data(reranker_result, prob_result, alpha):\n",
    "    k1 = alpha\n",
    "    k2 = 1 - alpha\n",
    "    result = []\n",
    "    for claim_index in range(len(reranker_result)):\n",
    "        doc_ids = reranker_result[claim_index]['doc_ids']\n",
    "        relevance_scores = reranker_result[claim_index]['scores']\n",
    "        prob_list = prob_result[claim_index]['evidence']\n",
    "        evidence_data = []\n",
    "        for doc_for_one_claim in range(len(doc_ids)):\n",
    "            doc_id = doc_ids[doc_for_one_claim]\n",
    "            relevance_score = relevance_scores[doc_for_one_claim]\n",
    "            relevance_score = sigmoid(relevance_score)*2 #because the original output is (-infinite，0)，we should map it to (0,1) as relevance\n",
    "            prob = prob_list[str(doc_id)]['probability']\n",
    "            prob = softmax(prob)\n",
    "            # max_prob = k1 * (max(prob[0], prob[2]) -  min(prob[0], prob[2])) - k2 * prob[1]\n",
    "            max_prob = 1 - prob[1]\n",
    "            fixed_score = k1*relevance_score + k2*max_prob\n",
    "            # fixed_score = relevance_score\n",
    "\n",
    "            evidence_data.append((doc_id, fixed_score, relevance_score, max_prob))\n",
    "\n",
    "        evidence_data_sorted = sorted(evidence_data, key=lambda x: x[1], reverse=True)\n",
    "        sorted_evidence = [key for key, _ , _ , _ in evidence_data_sorted]\n",
    "        sorted_scores = [score for _ , score, _ , _ in evidence_data_sorted]\n",
    "        sorted_relevance_scores = [relevance for _ , _ , relevance , _ in evidence_data_sorted]\n",
    "        sorted_prob = [prob for _ , _ , _ , prob  in evidence_data_sorted]\n",
    "        \n",
    "        one_result = {}\n",
    "        one_result[\"id\"] = reranker_result[claim_index]['id']\n",
    "        one_result[\"claim\"] = reranker_result[claim_index]['claim']\n",
    "        one_result[\"doc_ids\"] = sorted_evidence\n",
    "        one_result[\"scores\"] = sorted_scores\n",
    "        one_result[\"relevance_scores\"] = sorted_relevance_scores\n",
    "        one_result[\"prob_scores\"] = sorted_prob\n",
    "        result.append(one_result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4deb5aae-f1b0-4d50-8b64-e4be155a5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scifact(data_list):\n",
    "    data_list_copy = copy.deepcopy(data_list)\n",
    "    \n",
    "    for item in data_list_copy:\n",
    "        filtered_evidence = {k: v for k, v in item['evidence'].items() if v['provenance'] == 'citation'}\n",
    "        item['evidence'] = filtered_evidence\n",
    "        \n",
    "    return data_list_copy\n",
    "    \n",
    "def filter_open(data_list):\n",
    "    data_list_copy = copy.deepcopy(data_list)\n",
    "    \n",
    "    for item in data_list_copy:\n",
    "        filtered_evidence = {k: v for k, v in item['evidence'].items() if v['provenance'] == 'pooling'}\n",
    "        item['evidence'] = filtered_evidence\n",
    "        \n",
    "    return data_list_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a73547b-c866-412a-b0b8-a7516c30239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit(claim_set, predicted_set):\n",
    "    for top_k in [50,20,10,5,3,1]:\n",
    "        eval_dataset = {data['id']: data for data in claim_set}\n",
    "        hit_one = 0\n",
    "        hit_all = 0\n",
    "        total = 0\n",
    "        hit_one_evi = 0\n",
    "        hit_all_evi = 0\n",
    "        total_has_evi = 0\n",
    "        for retrieval in predicted_set:\n",
    "            total += 1\n",
    "            try:\n",
    "                data = eval_dataset[retrieval['id']]\n",
    "            except KeyError:\n",
    "                print('error')\n",
    "            pred_doc_ids = set(retrieval['doc_ids'][:top_k])\n",
    "            true_doc_ids = set(map(int, data['evidence'].keys()))\n",
    "        \n",
    "            if pred_doc_ids.intersection(true_doc_ids) or not true_doc_ids:\n",
    "                hit_one += 1\n",
    "\n",
    "            if pred_doc_ids.issuperset(true_doc_ids):\n",
    "                hit_all += 1\n",
    "            if true_doc_ids:\n",
    "                total_has_evi += 1\n",
    "                if pred_doc_ids.intersection(true_doc_ids):\n",
    "                    hit_one_evi += 1\n",
    "                if pred_doc_ids.issuperset(true_doc_ids):\n",
    "                    hit_all_evi += 1\n",
    "\n",
    "        hit_one_evidence = round(hit_one_evi / total_has_evi, 4)\n",
    "        hit_all_evidence = round(hit_all_evi / total_has_evi, 4)\n",
    "        \n",
    "        hit_one = round(hit_one / total, 4)\n",
    "        hit_all = round(hit_all / total, 4)\n",
    "    \n",
    "        result = hit_one_evidence, hit_all_evidence, hit_one, hit_all\n",
    "\n",
    "        print(result)\n",
    "\n",
    "    return hit_one_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2e3a01-ffe2-4fc0-9415-076e6e1f8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(claim_set, predicted_set):\n",
    "    for recall_k in [50,20,10,5,3,1]:\n",
    "        eval_dataset = {data['id']: data for data in claim_set}\n",
    "        total_evidence = 0\n",
    "        correct_retrieved = 0\n",
    "        for retrieval in predicted_set:\n",
    "            try:\n",
    "                data = eval_dataset[retrieval['id']]\n",
    "            except KeyError:\n",
    "                print('KeyError: claim_id not found')\n",
    "            pred_doc_ids = set(retrieval['doc_ids'][:recall_k])   \n",
    "            true_doc_ids = set(map(int, data['evidence'].keys()))\n",
    "            correct_retrieved += len(pred_doc_ids.intersection(true_doc_ids))\n",
    "            total_evidence += len(list(data['evidence'].keys()))\n",
    "        recall_at_k = round(correct_retrieved / total_evidence,4)\n",
    "\n",
    "        \n",
    "        print(f\"Recall@{recall_k}: {recall_at_k}\")\n",
    "    \n",
    "    return recall_at_k\n",
    "\n",
    "def calculate_precision_at_k(claim_set, predicted_set):\n",
    "    for precision_k in [50, 20, 10, 5, 3, 1]:\n",
    "        eval_dataset = {data['id']: data for data in claim_set}\n",
    "        total_retrieved = 0\n",
    "        correct_retrieved = 0\n",
    "        for retrieval in predicted_set:\n",
    "            try:\n",
    "                data = eval_dataset[retrieval['id']]\n",
    "            except KeyError:\n",
    "                print('KeyError: claim_id not found')\n",
    "                continue \n",
    "            pred_doc_ids = set(retrieval['doc_ids'][:precision_k])   \n",
    "            true_doc_ids = set(map(int, data['evidence'].keys()))\n",
    "            correct_retrieved += len(pred_doc_ids.intersection(true_doc_ids))\n",
    "            total_retrieved += min(len(retrieval['doc_ids']), precision_k)\n",
    "        \n",
    "        precision_at_k = round(correct_retrieved / total_retrieved, 4) if total_retrieved > 0 else 0.0\n",
    "        print(f\"Precision@{precision_k}: {precision_at_k}\")\n",
    "    \n",
    "    return precision_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea5049e-74c3-40ce-b1a7-a8037052b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_path_train = \"dataset/scifact_open/claims.jsonl\"\n",
    "claim_set_train = list(jsonlines.open(claim_path_train))\n",
    "citation = filter_scifact(claim_set_train)\n",
    "pooling = filter_open(claim_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83b8f1e-9b3b-4ced-bf7b-e3de1c81a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = \"document_retrieval_result/rerank_t5/t5_scifact_open_2000.jsonl\"\n",
    "rank_t5 = list(jsonlines.open(t5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21a608d-dfe5-4fdb-94d9-fa1024481294",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_result =  \"result_summary/prob_n5_scifact_open.jsonl\"\n",
    "prob_result = list(jsonlines.open(prob_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15626d6-39de-4c47-83fb-8c4c4c026913",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = mix_rerank_processed_data(rank_t5,prob_result,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64d0797-8aaf-4811-995b-a94e10432b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@50: 0.9234\n",
      "Recall@20: 0.8852\n",
      "Recall@10: 0.8565\n",
      "Recall@5: 0.8134\n",
      "Recall@3: 0.7321\n",
      "Recall@1: 0.5598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5598"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_recall_at_k(citation, combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a4b4e5-aa05-423f-9163-3aa3f3e3b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@50: 0.9163\n",
      "Recall@20: 0.7649\n",
      "Recall@10: 0.5936\n",
      "Recall@5: 0.4701\n",
      "Recall@3: 0.3586\n",
      "Recall@1: 0.1195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_recall_at_k(pooling, combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0383b-bccd-4ab9-821f-437d20ff4b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ea02f0-fc76-4219-8b93-e99c533eb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/dev data for +VeriRel\n",
    "# Here is an example using ComboScorer in SciFact-Open's documents.\n",
    "# +VeriRel in paper is trained on SciFact only, including 809 cliams with documents in SciFact corpus, as shown in an example file.\n",
    "\n",
    "target = combo\n",
    "claim_path_train = \"dataset/scifact/claims_train.jsonl\"\n",
    "# ouput_path = \"reranker_train_data/train_n5_verirel.jsonl\"\n",
    "\n",
    "claim_set_train = list(jsonlines.open(claim_path_train))\n",
    "\n",
    "\n",
    "output_data = []\n",
    "\n",
    "for item_idx in range(len(target)):\n",
    "    id = target[item_idx]['id']\n",
    "    claim = target[item_idx]['claim']\n",
    "    doc_ids = target[item_idx]['doc_ids'][:20]\n",
    "    prf_scores = target[item_idx]['scores'][:20]\n",
    "\n",
    "    gold_evi = list(claim_set_train[item_idx]['evidence'].keys())\n",
    "\n",
    "    for doc_idx in range(len(doc_ids)):\n",
    "        if str(doc_ids[doc_idx]) in gold_evi:\n",
    "            prf_scores[doc_idx] = 1\n",
    "            gold_evi.remove(str(doc_ids[doc_idx]))\n",
    "\n",
    "    for extra_evi in gold_evi:\n",
    "        doc_ids.append(int(extra_evi))\n",
    "        prf_scores.append(1)\n",
    "\n",
    "    output_data.append({\n",
    "        'id': id,\n",
    "        'claim': claim,\n",
    "        'doc_ids': doc_ids,\n",
    "        'prf_scores': prf_scores\n",
    "    })\n",
    "\n",
    "# with jsonlines.open(ouput_path, 'w') as writer:\n",
    "#     writer.write_all(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b21e0e-020f-4bcc-89f9-4279240141dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 8,\n",
       " 'claim': '25% of patients with melanoma and an objective response to PD-1 blockade will experience a progression in their melanoma.',\n",
       " 'doc_ids': [90543925,\n",
       "  46759314,\n",
       "  85563812,\n",
       "  3471191,\n",
       "  16869160,\n",
       "  4535882,\n",
       "  4468861,\n",
       "  16151191,\n",
       "  27452674,\n",
       "  73448986,\n",
       "  8443224,\n",
       "  4430143,\n",
       "  20634012,\n",
       "  52896012,\n",
       "  37970308,\n",
       "  73496636,\n",
       "  58602640,\n",
       "  13758726,\n",
       "  25967339,\n",
       "  208190996,\n",
       "  13734012],\n",
       " 'prf_scores': [0.8430081279765058,\n",
       "  0.8069977897657188,\n",
       "  0.7339445404429519,\n",
       "  0.7284783915761297,\n",
       "  0.701802160556745,\n",
       "  0.6954451328224922,\n",
       "  0.5576161104618667,\n",
       "  0.5490565537207442,\n",
       "  0.5452018016621938,\n",
       "  0.529645609493147,\n",
       "  0.5235234311643397,\n",
       "  0.5022624192675493,\n",
       "  0.49779895210932745,\n",
       "  0.495493161813086,\n",
       "  0.49500382823649736,\n",
       "  0.4948032039116302,\n",
       "  0.49388343836439896,\n",
       "  0.48632340028069526,\n",
       "  0.48132901319658183,\n",
       "  0.4659646362193301,\n",
       "  1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4062e682-16f4-4699-aeb2-3a11a2c68803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 38,\n",
       " 'claim': 'A deficiency of vitamin B6 increases blood levels of homocysteine.',\n",
       " 'doc_ids': [45829252,\n",
       "  11911440,\n",
       "  22556029,\n",
       "  35969491,\n",
       "  2326835,\n",
       "  5702170,\n",
       "  8294579,\n",
       "  4511158,\n",
       "  33409100,\n",
       "  16252863,\n",
       "  15834427,\n",
       "  4515153,\n",
       "  6084615,\n",
       "  12810152,\n",
       "  35765068,\n",
       "  207584290,\n",
       "  23481830,\n",
       "  10662555,\n",
       "  24278506,\n",
       "  5640510],\n",
       " 'prf_scores': [0.902665050508054,\n",
       "  0.8708963809070753,\n",
       "  0.8698555186254426,\n",
       "  0.8024508961400137,\n",
       "  0.797819553858055,\n",
       "  0.765661512688123,\n",
       "  0.7008841344217085,\n",
       "  0.6732646584018274,\n",
       "  1,\n",
       "  0.6219188457835844,\n",
       "  0.5662139375124435,\n",
       "  0.5643951489571466,\n",
       "  0.5605270635818482,\n",
       "  0.5512924270833537,\n",
       "  0.5438508768769463,\n",
       "  0.5366398960834721,\n",
       "  0.5223664523460679,\n",
       "  0.46648852831982635,\n",
       "  0.4561933331730313,\n",
       "  0.4511174614591327]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data[7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank",
   "language": "python",
   "name": "rerank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
